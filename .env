# Pascal AI Assistant - Simplified Environment Configuration (v3.0)
# Optimized for Pi 5 with Groq + Ollama only

# üöÄ GROQ API (Primary and Only Online Provider)
# Get from: https://console.groq.com/
# IMPORTANT: New format uses gsk_ (underscore, not dash)
GROQ_API_KEY=gsk_your_actual_groq_api_key_here

# ‚ö° PERFORMANCE SETTINGS (Optimized for Pi 5)
PERFORMANCE_MODE=balanced
TARGET_RESPONSE_TIME=2.0
STREAMING_ENABLED=true
KEEP_ALIVE_ENABLED=true
MAX_RESPONSE_TOKENS=200
TEMPERATURE=0.7

# ü¶ô OLLAMA SETTINGS
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=30
OLLAMA_KEEP_ALIVE=30m

# üíæ MEMORY SETTINGS  
MEMORY_LIMIT=5
CONTEXT_WINDOW=1024

# üêõ DEBUG SETTINGS
DEBUG=false
LOG_LEVEL=INFO

# üìù SETUP INSTRUCTIONS:
# 1. Copy this file to .env
# 2. Get Groq API key from https://console.groq.com/
# 3. Replace gsk_your_actual_groq_api_key_here with your actual key
# 4. Make sure key starts with gsk_ (underscore format)
# 5. Start Ollama: sudo systemctl start ollama  
# 6. Download model: ollama pull nemotron-mini:4b-instruct-q4_K_M
# 7. Run Pascal: python main.py

# üéØ PERFORMANCE TARGETS:
# - Current info queries: 1-3 seconds (via Groq)
# - General queries: 0.5-2 seconds (via Ollama)
# - Simple greetings: < 1 second
# - Streaming responses for instant feedback

# üîß TROUBLESHOOTING:
# - If "online not available": Check GROQ_API_KEY format and validity
# - If "offline not available": Check Ollama status with 'sudo systemctl status ollama'
# - For slow responses: Check Pi temperature with 'vcgencmd measure_temp'
# - For current info issues: Ensure internet connection and valid Groq API key
