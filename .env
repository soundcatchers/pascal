# Pascal AI Assistant Environment Variables - Lightning Version with Groq Priority
# Copy this file to .env and add your actual API keys

# ‚ö° LIGHTNING PERFORMANCE SETTINGS
PERFORMANCE_MODE=speed              # speed/balanced/quality (default: speed for lightning)
STREAMING_ENABLED=true             # Enable streaming for instant feedback
KEEP_ALIVE_ENABLED=true           # Keep models loaded in memory
TARGET_RESPONSE_TIME=3.0          # Target max response time in seconds
MAX_RESPONSE_TOKENS=200           # Limit tokens for faster responses

# ü§ñ ONLINE API KEYS (Priority Order: Groq -> Gemini -> OpenAI)

# Groq API (PRIMARY - Very Fast) - Get from https://console.groq.com/
# Format: gsk-... (starts with gsk-)
GROQ_API_KEY=gsk-your_groq_api_key_here

# Google Gemini API (SECONDARY - Free & Good Quality) - Get from https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here
# Alternative: You can also use GOOGLE_API_KEY if you prefer
# GOOGLE_API_KEY=your_google_api_key_here

# OpenAI API (FALLBACK - Reliable but Paid) - Get from https://platform.openai.com/api-keys
# Format: sk-... (starts with sk-)
OPENAI_API_KEY=sk-your_openai_api_key_here

# üîß OLLAMA SETTINGS
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=30
OLLAMA_KEEP_ALIVE=5m              # Keep models loaded for 5 minutes

# üíæ MEMORY SETTINGS
SHORT_TERM_MEMORY_LIMIT=5
LONG_TERM_MEMORY_ENABLED=true
MEMORY_SAVE_INTERVAL=300

# üêõ DEBUG SETTINGS
DEBUG=false
LOG_LEVEL=INFO
PERF_LOG=false                    # Enable performance logging

# ‚öôÔ∏è ADVANCED SETTINGS
MAX_CONCURRENT_REQUESTS=2         # Limited for Pi 5 stability
CACHE_EXPIRY=3600
LLM_THREADS=4                     # Use all Pi 5 cores
LLM_CONTEXT=2048                  # Context window size

# üéØ PREFERRED MODELS (in priority order)
# These are automatically downloaded and configured
# 1. nemotron-mini:4b-instruct-q4_K_M (PRIMARY - fastest)
# 2. llama3.2:3b (FALLBACK 1 - current Llama)
# 3. qwen2.5:3b (FALLBACK 2 - updated Qwen)
# 4. phi3:mini (FALLBACK 3 - reliable)
# 5. gemma2:2b (FALLBACK 4 - lightweight)

# üìù API PRIORITY EXPLANATION:
# 1. GROQ (GROQ_API_KEY) - Extremely fast inference with current models:
#    - llama-3.1-8b-instant (primary, fastest)
#    - llama-3.1-70b-versatile (high quality)
#    - llama-3.2-11b-text-preview (balanced)
#    - gemma2-9b-it (Google model on Groq)
# 2. GEMINI - Free tier available, good quality responses
#    - gemini-2.0-flash-exp (primary)
#    - gemini-1.5-flash (fallback)
# 3. OPENAI - Reliable fallback, requires paid credits
#    - gpt-4o-mini (primary)
#    - gpt-4o (high quality)

# üîë HOW TO GET API KEYS:
# Groq (Recommended for speed):
#   1. Go to https://console.groq.com/
#   2. Sign up for free account
#   3. Generate API key (starts with gsk-)
#   4. Replace gsk-your_groq_api_key_here with your actual key
#
# Gemini (Free option):
#   1. Go to https://aistudio.google.com/app/apikey
#   2. Sign in with Google account
#   3. Create API key
#   4. Replace your_gemini_api_key_here with your actual key
#
# OpenAI (Paid but reliable):
#   1. Go to https://platform.openai.com/api-keys
#   2. Sign up and add payment method
#   3. Create API key (starts with sk-)
#   4. Replace sk-your_openai_api_key_here with your actual key

# üí° RECOMMENDATION:
# For best performance, configure GROQ_API_KEY first.
# Groq provides lightning-fast responses and is free for most usage.
# Add GEMINI_API_KEY as a free backup option.
# Only add OPENAI_API_KEY if you need maximum reliability and don't mind paying.
