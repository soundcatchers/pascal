Adjustments can be made to how the offline llm behaves with regards to response length and also the mount of tokens for each response...

Edit modules/offline_llm.py

Additional Options You Can Tweak:

If you want to further customize the behavior, you can edit these settings in the file:

1.Timeout Configuration (line ~67):

pythonself.timeout_config = {
    'simple': 15.0,    # Adjust for simple queries
    'medium': 25.0,    # Adjust for medium complexity
    'complex': 40.0,   # Adjust for complex queries
    'default': 20.0    # Default timeout
}

2.Token Limits (lines ~75-95):

python'num_predict': 100,  # Simple queries (make smaller for more concise)
'num_predict': 200,  # Medium queries
'num_predict': 300,  # Complex queries (make larger for more detail)

3. Add Your Own Keywords (line ~110):

pythoncomplex_keywords = ['explain', 'analyze', ...]  # Add topics that need detail
simple_keywords = ['hi', 'hello', ...]  # Add queries that should be quick

Alternative Quick Fix:

1.If you just want a simple global adjustment without the smart analysis, you can:

For more concise responses everywhere, edit the config:

python'num_predict': 150,  # Reduce from current values

2.For longer timeout everywhere, edit line ~60:

pythontimeout = aiohttp.ClientTimeout(total=90, sock_connect=5, sock_read=60)

Benefits of This Approach:

1.No more premature timeouts on complex questions
2.Faster, more concise responses for simple queries
3.Adaptive behavior - the system learns what type of answer you need
4.Better user experience - graceful handling instead of error messages
5.Maintains speed where it matters (simple queries stay fast)


Keep-Alive Configuration Explained
What It Does
The keep-alive system keeps your Ollama model loaded in RAM even when Pascal isn't actively generating responses. This is like keeping an app open in the background rather than closing and reopening it each time.
The Two Settings:
pythonself.keep_alive_interval = 30  # seconds
self.keep_alive_duration = "5m"  # Keep model loaded for 5 minutes

keep_alive_interval (30 seconds)

This is how often Pascal "pings" Ollama to say "keep the model loaded"
Every 30 seconds, Pascal sends a keep-alive request to Ollama
This happens in the background via the _keep_alive_loop() function


keep_alive_duration ("5m")

This tells Ollama: "After I finish using the model, keep it in memory for 5 more minutes"
If no requests come in for 5 minutes, Ollama will unload the model to free up RAM
Format can be: "5m" (minutes), "30s" (seconds), "1h" (hour), or "-1" (keep forever)



How It Works in Practice:
User asks question → Model loads (if not loaded) → Response generated
                                ↓
                    Model stays loaded for 5 minutes
                                ↓
        Background task pings every 30 seconds to extend the 5-minute window
                                ↓
            User asks another question → Instant response (no loading!)
The Background Keep-Alive Loop:
pythonasync def _keep_alive_loop(self):
    """Background task to keep model loaded in memory"""
    while self.model_loaded and self.current_model:
        await asyncio.sleep(self.keep_alive_interval)  # Wait 30 seconds
        
        # Send empty request to keep model alive
        payload = {
            "model": self.current_model.name,
            "keep_alive": self.keep_alive_duration,  # Reset 5-minute timer
            "prompt": "",
            "stream": False
        }
        
        async with self.session.post(
            f"{self.ollama_host}/api/generate",
            json=payload
        ) as response:
            # This empty request just keeps the model loaded
Performance Impact:
Without Keep-Alive:

First response: 10-15 seconds (model loads from disk)
Second response: 10-15 seconds (model loads again)
Every response has loading overhead

With Keep-Alive:

First response: 10-15 seconds (initial load)
Second response: 1-3 seconds (model already in RAM!)
Subsequent responses stay fast

Memory Trade-offs:
Your models use approximately:

nemotron-mini: ~2.7GB RAM when loaded
qwen3:4b: ~2.5GB RAM when loaded
phi3:mini: ~2.2GB RAM when loaded

With 16GB RAM on your Pi 5, keeping one model loaded leaves plenty of RAM for the system.
Customization Options:
You can adjust these values based on your usage:
For frequent use (keep model loaded longer):
pythonself.keep_alive_interval = 60  # Check less frequently
self.keep_alive_duration = "30m"  # Keep for 30 minutes
For occasional use (free RAM sooner):
pythonself.keep_alive_interval = 30  # Keep current
self.keep_alive_duration = "2m"  # Unload after 2 minutes
For always loaded (maximum speed, uses constant RAM):
pythonself.keep_alive_interval = 60
self.keep_alive_duration = "-1"  # Never unload
To disable keep-alive (save RAM but slower):
pythonself.keep_alive_duration = "0"  # Unload immediately
# And comment out the keep_alive_task creation
You Can Also Set It Globally in Ollama:
Edit Ollama's service configuration:
bashsudo systemctl edit ollama
Add:
[Service]
Environment="OLLAMA_KEEP_ALIVE=10m"
This sets a system-wide default of 10 minutes.
Monitor Your Setup:
Check if a model is loaded:
bashollama ps
This shows currently loaded models and their memory usage.
Recommendation for Your Pi 5:
With 16GB RAM, your current settings are good:

30-second interval doesn't waste resources
5-minute duration is reasonable for conversational use
The model unloads automatically if you're away

If you use Pascal very frequently throughout the day, you might want to increase to "15m" or even "30m" to avoid reloading. If you use it in bursts (lots of questions, then nothing for hours), the current 5 minutes is perfect.
